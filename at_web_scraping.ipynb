{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\giova\\AppData\\Local\\Temp\\ipykernel_12984\\4018586642.py\", line 1, in <module>\n",
      "    import pandas as pd\n",
      "  File \"c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\__init__.py\", line 39, in <module>\n",
      "    from pandas.compat import (\n",
      "  File \"c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\compat\\__init__.py\", line 27, in <module>\n",
      "    from pandas.compat.pyarrow import (\n",
      "  File \"c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\compat\\pyarrow.py\", line 8, in <module>\n",
      "    import pyarrow as pa\n",
      "  File \"c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyarrow\\__init__.py\", line 65, in <module>\n",
      "    import pyarrow.lib as _lib\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\giova\\AppData\\Local\\Temp\\ipykernel_12984\\4018586642.py\", line 1, in <module>\n",
      "    import pandas as pd\n",
      "  File \"c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\__init__.py\", line 62, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\api.py\", line 9, in <module>\n",
      "    from pandas.core.dtypes.dtypes import (\n",
      "  File \"c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\dtypes\\dtypes.py\", line 24, in <module>\n",
      "    from pandas._libs import (\n",
      "  File \"c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyarrow\\__init__.py\", line 65, in <module>\n",
      "    import pyarrow.lib as _lib\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercício 1: \n",
    "\n",
    "Baixe seu perfil no Linkedin em PDF e utilize o PyPDF2 para construir uma função que retorne a string do texto completo do documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q PyPDF2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criaremos uma função para extrair o PDF, essa função ficará salva na pasta scripts\n",
    "\n",
    "import PyPDF2\n",
    "\n",
    "def extrair_pdf(path_pdf):\n",
    "    texto_completo = \"\"\n",
    "    \n",
    "    # Abre o arquivo PDF \n",
    "    with open(path_pdf, 'rb') as arquivo_pdf:\n",
    "        leitor_pdf = PyPDF2.PdfReader(arquivo_pdf)\n",
    "        \n",
    "        # Itera sobre todas as páginas do PDF\n",
    "        for pagina in leitor_pdf.pages:\n",
    "            texto_pagina = pagina.extract_text()\n",
    "            if texto_pagina:\n",
    "                texto_completo += texto_pagina\n",
    "    \n",
    "    return texto_completo\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Chamando a função"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto salvo em: C:/infnet_ultimo_semestre/at_web_scraping/documents/texto_extraido.txt\n"
     ]
    }
   ],
   "source": [
    "# Caminho do arquivo PDF (em documents)\n",
    "caminho_pdf = r\"C:/infnet_ultimo_semestre/at_web_scraping/documents/linkedin_profile.pdf\"\n",
    "\n",
    "# Chamando a função p\n",
    "texto_do_pdf = extrair_pdf(caminho_pdf)\n",
    "\n",
    "# Caminho para salvar o arquivo de texto\n",
    "caminho_arquivo_saida = r\"C:/infnet_ultimo_semestre/at_web_scraping/documents/texto_extraido.txt\"\n",
    "\n",
    "# Salva o texto extraído em um arquivo txt\n",
    "with open(caminho_arquivo_saida, 'w', encoding='utf-8') as arquivo_saida:\n",
    "    arquivo_saida.write(texto_do_pdf)\n",
    "\n",
    "print(f\"Texto salvo em: {caminho_arquivo_saida}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercício 2:\n",
    "\n",
    "Utilize Regex (módulo `re` nativo do Python) para criar uma função que, a partir do texto extraído, retorne um dicionário com as seguintes informações: \n",
    "\n",
    "Seu número de telefone;\n",
    "Seu endereço de email; e \n",
    "O link do seu perfil no Linkedin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def extrair_informacoes(texto):\n",
    "    # Remover quebras de linha e espaços extras\n",
    "    texto = texto.replace('\\n', ' ').replace('  ', ' ')\n",
    "    \n",
    "    # Padrões regex para telefone, email e Linkedin\n",
    "    telefone_regex = r\"\\(?\\+?\\d{0,3}\\)?\\s?\\(?\\d{2,3}\\)?\\s?\\d{4,5}-?\\d{4}\"\n",
    "    email_regex = r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\"\n",
    "    linkedin_regex = r\"www\\.linkedin\\.com/in/[a-zA-Z0-9_-]+\"\n",
    "    \n",
    "    # Aplicando os regex\n",
    "    telefone = re.search(telefone_regex, texto)\n",
    "    email = re.search(email_regex, texto)\n",
    "    linkedin = re.search(linkedin_regex, texto)\n",
    "    \n",
    "    # Caso não houver telefone retorna \"NaN\" (meu caso)\n",
    "    telefone_resultado = telefone.group(0).strip() if telefone else np.nan\n",
    "    \n",
    "    # Captura correta do e-mail e Linkedin, removendo quebras de linha e espaços extras\n",
    "    email_resultado = email.group(0).strip() if email else np.nan\n",
    "    linkedin_resultado = linkedin.group(0).strip() if linkedin else np.nan\n",
    "    \n",
    "    # Armazenar as saídas no dicionário\n",
    "    informacoes = {\n",
    "        \"telefone\": telefone_resultado,\n",
    "        \"email\": email_resultado,\n",
    "        \"linkedin\": linkedin_resultado\n",
    "    }\n",
    "    \n",
    "    return informacoes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Informações extraídas salvas\n"
     ]
    }
   ],
   "source": [
    "# Chamando a função e salvando o arquivo na mesma pasta\n",
    "\n",
    "# Lendo o texto salvo no arquivo texto_extraido.txt\n",
    "with open(r'C:\\infnet_ultimo_semestre\\at_web_scraping\\documents\\texto_extraido.txt', 'r', encoding='utf-8') as file:\n",
    "    texto_extraido = file.read()\n",
    "\n",
    "# Função para extrair as informações\n",
    "informacoes_extraidas = extrair_informacoes(texto_extraido)\n",
    "\n",
    "# Salvando as informações extraídas \n",
    "with open(r'C:\\infnet_ultimo_semestre\\at_web_scraping\\documents\\informacoes_extraidas.txt', 'w', encoding='utf-8') as file:\n",
    "    for chave, valor in informacoes_extraidas.items():\n",
    "        file.write(f'{chave}: {valor}\\n')\n",
    "\n",
    "print(\"Informações extraídas salvas\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercício 3:\n",
    "\n",
    "Aplique as funções geradas nas questões 1 e 2 para fazer o mesmo com o PDF em anexo (perfil do professor) e crie um CSV com todas as informações extraídas (colunas: nome, telefone, email e perfil, tanto do PDF de vocês quanto do fornecido) utilizando o módulo `csv` nativo do Python. Obs.: ao final os padrões utilizados no Regex devem abarcar os conteúdos dos dois PDFs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Obs:** Aplicaremos as mesmas funções apenas sobre o perfil do professor Vinicius, em seguida, concatenaremos os dois arquivos .txt em um único arquivo csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Parte 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo txt do salvo em: C:/infnet_ultimo_semestre/at_web_scraping/documents/texto_extraido_professor.txt\n",
      "Informações salvas.\n"
     ]
    }
   ],
   "source": [
    "# Caminho do PDF do professor (em documents)\n",
    "caminho_pdf_professor = r\"C:/infnet_ultimo_semestre/at_web_scraping/documents/ViniciusBS_Perfil.pdf\"\n",
    "\n",
    "# Extrair o texto do PDF \n",
    "texto_do_pdf_professor = extrair_pdf(caminho_pdf_professor)\n",
    "\n",
    "# Caminho para salvar o arquivo de texto \n",
    "caminho_arquivo_saida_professor = r\"C:/infnet_ultimo_semestre/at_web_scraping/documents/texto_extraido_professor.txt\"\n",
    "\n",
    "# Salva o texto extraído em um arquivo txt\n",
    "with open(caminho_arquivo_saida_professor, 'w', encoding='utf-8') as arquivo_saida:\n",
    "    arquivo_saida.write(texto_do_pdf_professor)\n",
    "\n",
    "print(f\"Arquivo txt do salvo em: {caminho_arquivo_saida_professor}\")\n",
    "\n",
    "# Lendo o texto salvo no arquivo texto_extraido_professor.txt\n",
    "with open(r'C:\\infnet_ultimo_semestre\\at_web_scraping\\documents\\texto_extraido_professor.txt', 'r', encoding='utf-8') as file:\n",
    "    texto_extraido_professor = file.read()\n",
    "\n",
    "# Extrair as informações do texto do professor com a segunda função\n",
    "informacoes_extraidas_professor = extrair_informacoes(texto_extraido_professor)\n",
    "\n",
    "# Salvando as informações extraídas \n",
    "with open(r'C:\\infnet_ultimo_semestre\\at_web_scraping\\documents\\informacoes_extraidas_professor.txt', 'w', encoding='utf-8') as file:\n",
    "    for chave, valor in informacoes_extraidas_professor.items():\n",
    "        file.write(f'{chave}: {valor}\\n')\n",
    "\n",
    "print(\"Informações salvas.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Parte 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Informações combinadas salvas no arquivo CSV: C:\\infnet_ultimo_semestre\\at_web_scraping\\documents\\informacoes_perfis_combinadas.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Abrindo o arquivo de informações extraídas do seu perfil\n",
    "with open(r'C:\\infnet_ultimo_semestre\\at_web_scraping\\documents\\informacoes_extraidas.txt', 'r', encoding='utf-8') as file:\n",
    "    informacoes_perfil1 = file.read()\n",
    "\n",
    "# Abrindo o arquivo de informações extraídas do professor\n",
    "with open(r'C:\\infnet_ultimo_semestre\\at_web_scraping\\documents\\informacoes_extraidas_professor.txt', 'r', encoding='utf-8') as file:\n",
    "    informacoes_perfil2 = file.read()\n",
    "\n",
    "# Função auxiliar para converter as informações de string para dicionário\n",
    "def extrair_dados_do_texto(texto):\n",
    "    linhas = texto.splitlines()\n",
    "    dados = {}\n",
    "    for linha in linhas:\n",
    "        chave, valor = linha.split(\": \", 1)\n",
    "        dados[chave] = valor.strip()  # Remover possíveis espaços\n",
    "    return dados\n",
    "\n",
    "# Processando as informações extraídas\n",
    "dados_perfil1 = extrair_dados_do_texto(informacoes_perfil1)\n",
    "dados_perfil2 = extrair_dados_do_texto(informacoes_perfil2)\n",
    "\n",
    "# Nome das colunas do CSV\n",
    "colunas = ['nome', 'telefone', 'email', 'linkedin']\n",
    "\n",
    "# Dados a serem escritos no CSV\n",
    "dados = [\n",
    "    ['Giovano Panatta', dados_perfil1.get('telefone', np.nan), dados_perfil1.get('email', np.nan), dados_perfil1.get('linkedin', np.nan)],\n",
    "    ['Vinicius Branco', dados_perfil2.get('telefone', np.nan), dados_perfil2.get('email', np.nan), dados_perfil2.get('linkedin', np.nan)]\n",
    "]\n",
    "\n",
    "# Definir o caminho onde o CSV será salvo\n",
    "caminho_csv = r'C:\\infnet_ultimo_semestre\\at_web_scraping\\documents\\informacoes_perfis_combinadas.csv'\n",
    "\n",
    "# Salvando os dados no CSV\n",
    "with open(caminho_csv, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(colunas)  # Escreve os cabeçalhos\n",
    "    writer.writerows(dados)   # Escreve os dados\n",
    "\n",
    "print(f\"Informações combinadas salvas no arquivo CSV: {caminho_csv}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercício 4:\n",
    "\n",
    "Explore o “playground” da API do SimilarWeb encontrada no RapidAPI (https://rapidapi.com/Glavier/api/similarweb12/playground/) e inscreva-se no plano gratuito, então crie um código para obter os dados dos 10 primeiros sites listados em “top-websites”, salvando-os em um dataframe do Pandas e enfim em um arquivo CSV usando o próprio Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>favicon</th>\n",
       "      <th>rankChange</th>\n",
       "      <th>categoryId</th>\n",
       "      <th>visitsAvgDurationFormatted</th>\n",
       "      <th>pagesPerVisit</th>\n",
       "      <th>bounceRate</th>\n",
       "      <th>isBlackListed</th>\n",
       "      <th>isNewRank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>google.com</td>\n",
       "      <td>https://site-images.similarcdn.com/image?url=g...</td>\n",
       "      <td>0</td>\n",
       "      <td>computers_electronics_and_technology/search_en...</td>\n",
       "      <td>00:10:45</td>\n",
       "      <td>8.146449</td>\n",
       "      <td>0.284144</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>youtube.com</td>\n",
       "      <td>https://site-images.similarcdn.com/image?url=y...</td>\n",
       "      <td>0</td>\n",
       "      <td>arts_and_entertainment/tv_movies_and_streaming</td>\n",
       "      <td>00:20:10</td>\n",
       "      <td>10.701296</td>\n",
       "      <td>0.238583</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>facebook.com</td>\n",
       "      <td>https://site-images.similarcdn.com/image?url=f...</td>\n",
       "      <td>0</td>\n",
       "      <td>computers_electronics_and_technology/social_ne...</td>\n",
       "      <td>00:11:03</td>\n",
       "      <td>11.576611</td>\n",
       "      <td>0.304578</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>instagram.com</td>\n",
       "      <td>https://site-images.similarcdn.com/image?url=i...</td>\n",
       "      <td>0</td>\n",
       "      <td>computers_electronics_and_technology/social_ne...</td>\n",
       "      <td>00:08:34</td>\n",
       "      <td>11.612183</td>\n",
       "      <td>0.350096</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>x.com</td>\n",
       "      <td>https://site-images.similarcdn.com/image?url=x...</td>\n",
       "      <td>0</td>\n",
       "      <td>computers_electronics_and_technology/social_ne...</td>\n",
       "      <td>00:11:53</td>\n",
       "      <td>12.281232</td>\n",
       "      <td>0.359107</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          domain                                            favicon  \\\n",
       "0     google.com  https://site-images.similarcdn.com/image?url=g...   \n",
       "1    youtube.com  https://site-images.similarcdn.com/image?url=y...   \n",
       "2   facebook.com  https://site-images.similarcdn.com/image?url=f...   \n",
       "3  instagram.com  https://site-images.similarcdn.com/image?url=i...   \n",
       "4          x.com  https://site-images.similarcdn.com/image?url=x...   \n",
       "\n",
       "   rankChange                                         categoryId  \\\n",
       "0           0  computers_electronics_and_technology/search_en...   \n",
       "1           0     arts_and_entertainment/tv_movies_and_streaming   \n",
       "2           0  computers_electronics_and_technology/social_ne...   \n",
       "3           0  computers_electronics_and_technology/social_ne...   \n",
       "4           0  computers_electronics_and_technology/social_ne...   \n",
       "\n",
       "  visitsAvgDurationFormatted  pagesPerVisit  bounceRate  isBlackListed  \\\n",
       "0                   00:10:45       8.146449    0.284144          False   \n",
       "1                   00:20:10      10.701296    0.238583          False   \n",
       "2                   00:11:03      11.576611    0.304578          False   \n",
       "3                   00:08:34      11.612183    0.350096          False   \n",
       "4                   00:11:53      12.281232    0.359107          False   \n",
       "\n",
       "   isNewRank  \n",
       "0      False  \n",
       "1      False  \n",
       "2      False  \n",
       "3      False  \n",
       "4      False  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importando o requests\n",
    "import requests\n",
    "\n",
    "\n",
    "# url do endpoint \n",
    "url = \"https://similarweb12.p.rapidapi.com/v3/top-websites/\"\n",
    "\n",
    "# Definomdp os headers com chave da API e host\n",
    "headers = {\n",
    "    \"x-rapidapi-key\": \"f3a760c9cemsh6ba622275e00ec8p16c00fjsnfb732aa83ed1\", \n",
    "    \"x-rapidapi-host\": \"similarweb12.p.rapidapi.com\"\n",
    "}\n",
    "\n",
    "# Fazer a requisição GET\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Verificar se a requisição foi bem-sucedida\n",
    "if response.status_code == 200: # caso ok, converter para json\n",
    "    data = response.json() \n",
    "    \n",
    "    # Extrair os dados dos 10 primeiros sites usando a chave 'sites'\n",
    "    top_sites = data['sites'][:10]\n",
    "    \n",
    "    # Criar um DF com os dados dos 10 primeiros sites\n",
    "    df = pd.DataFrame(top_sites)\n",
    "    \n",
    "\n",
    "df.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados dos 10 primeiros sites salvos no arquivo CSV: C:\\infnet_ultimo_semestre\\at_web_scraping\\documents\\top_websites.csv\n"
     ]
    }
   ],
   "source": [
    "# Definir o caminho para salvar o arquivo CSV\n",
    "caminho_csv = r'C:\\infnet_ultimo_semestre\\at_web_scraping\\documents\\top_websites.csv'\n",
    "\n",
    "# Salvar o DataFrame como um arquivo CSV\n",
    "df.to_csv(caminho_csv, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"Dados dos 10 primeiros sites salvos no arquivo CSV: {caminho_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercício 5:\n",
    "\n",
    "* Utilize o arquivo XML em anexo e a biblioteca `lxml` com caminhos relativos de XPath para:\n",
    "\n",
    "* Selecionar os nomes de todos *estudantes* que estejam no 2º ano ou acima dele;\n",
    "\n",
    "* Selecionar o nome do *professor* de Estruturas de Dados (course: \"Data Structures\");\n",
    "\n",
    "* Selecionar os títulos de todos os *cursos* ofertados pelo departamento de Ciência da Computação (department: Computer Science);\n",
    "\n",
    "* Selecionar os nomes de todos os *departamentos* que sejam pertencentes à Escola de Engenharia (college: Engineering)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<university>\n",
      "    <college name=\"Engineering\">\n",
      "        <department name=\"Computer Science\" head=\"Dr. Alan Smith\">\n",
      "            <course code=\"CS101\" credits=\"4\">\n",
      "                <professor tenure=\"true\">Dr. Jane Doe</professor>\n",
      "                <title>Introduction to Computer Science</title>\n",
      "                <schedule>\n",
      "                    <day>Monday</day>\n",
      "                    <day>Wednesday</day>\n",
      "                    <time>10:00 AM - 12:00 PM</time>\n",
      "                </schedule>\n",
      "                <students>\n",
      "                    <student id=\"1001\" grade=\"A\" year=\"2\">Alice</student>\n",
      "                    <student id=\"1002\" grade=\"B+\" year=\"2\">Bob</student>\n",
      "                </students>\n",
      "            </course>\n",
      "            <course code=\"CS202\" credits=\"3\">\n",
      "                <professor tenure=\"false\">Dr. Emily Clark</professor>\n",
      "                <title>Data Structures</title>\n",
      "                <schedule>\n",
      "                    <day>Tuesday</day>\n",
      "                    <day>Thursday</day>\n",
      "                    <time>1:00 PM - 3:00 PM</time>\n",
      "                </schedule>\n",
      "                <students>\n",
      "                    <student id=\"1003\" grade=\"B-\" year=\"3\">Charlie</student>\n",
      "                    <student id=\"1004\" grade=\"A-\" year=\"3\">David</student>\n",
      "                </students>\n",
      "            </course>\n",
      "        </department>\n",
      "        <department name=\"Mechanical Engineering\" head=\"Dr. Robert Miller\">\n",
      "            <course code=\"ME101\" credits=\"4\">\n",
      "                <professor tenure=\"true\">Dr. John Lee</professor>\n",
      "                <title>Thermodynamics</title>\n",
      "                <schedule>\n",
      "                    <day>Monday</day>\n",
      "                    <day>Wednesday</day>\n",
      "                    <time>9:00 AM - 11:00 AM</time>\n",
      "                </schedule>\n",
      "                <students>\n",
      "                    <student id=\"1005\" grade=\"B\" year=\"1\">Eva</student>\n",
      "                    <student id=\"1006\" grade=\"A+\" year=\"1\">Frank</student>\n",
      "                </students>\n",
      "            </course>\n",
      "            <course code=\"ME202\" credits=\"3\">\n",
      "                <professor tenure=\"false\">Dr. William Kim</professor>\n",
      "                <title>Fluid Mechanics</title>\n",
      "                <schedule>\n",
      "                    <day>Tuesday</day>\n",
      "                    <day>Thursday</day>\n",
      "                    <time>2:00 PM - 4:00 PM</time>\n",
      "                </schedule>\n",
      "                <students>\n",
      "                    <student id=\"1007\" grade=\"C\" year=\"2\">Grace</student>\n",
      "                    <student id=\"1008\" grade=\"B+\" year=\"2\">Henry</student>\n",
      "                </students>\n",
      "            </course>\n",
      "        </department>\n",
      "    </college>\n",
      "    <college name=\"Arts\">\n",
      "        <department name=\"History\" head=\"Dr. Susan Brown\">\n",
      "            <course code=\"HIS101\" credits=\"3\">\n",
      "                <professor tenure=\"true\">Dr. Anna White</professor>\n",
      "                <title>World History</title>\n",
      "                <schedule>\n",
      "                    <day>Monday</day>\n",
      "                    <day>Wednesday</day>\n",
      "                    <time>11:00 AM - 12:30 PM</time>\n",
      "                </schedule>\n",
      "                <students>\n",
      "                    <student id=\"1009\" grade=\"A\" year=\"2\">Isla</student>\n",
      "                    <student id=\"1010\" grade=\"B\" year=\"3\">Jack</student>\n",
      "                </students>\n",
      "            </course>\n",
      "        </department>\n",
      "        <department name=\"Philosophy\" head=\"Dr. Kevin Johnson\">\n",
      "            <course code=\"PHI201\" credits=\"4\">\n",
      "                <professor tenure=\"false\">Dr. Michael Green</professor>\n",
      "                <title>Ethics</title>\n",
      "                <schedule>\n",
      "                    <day>Tuesday</day>\n",
      "                    <day>Thursday</day>\n",
      "                    <time>9:00 AM - 11:00 AM</time>\n",
      "                </schedule>\n",
      "                <students>\n",
      "                    <student id=\"1011\" grade=\"A-\" year=\"1\">Liam</student>\n",
      "                    <student id=\"1012\" grade=\"B+\" year=\"1\">Mia</student>\n",
      "                </students>\n",
      "            </course>\n",
      "        </department>\n",
      "    </college>\n",
      "</university>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Carrega o arquivo XML\n",
    "xml_arquivo = 'C:/infnet_ultimo_semestre/at_web_scraping/documents/AT.xml'  \n",
    "\n",
    "# Parsear sobre XML\n",
    "tree = etree.parse(xml_arquivo)\n",
    "\n",
    "# Defini a raiz do XML para facilitar as consultas\n",
    "root = tree.getroot()\n",
    "\n",
    "# Embora possamos visualizar a estrutura no VS code, é uma boa prática visualizar via código\n",
    "print(etree.tostring(root, pretty_print=True).decode('utf-8'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Selecionar os nomes de todos *estudantes* que estejam no 2º ano ou acima dele;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Alice', 'Bob', 'Charlie', 'David', 'Grace', 'Henry', 'Isla', 'Jack']\n"
     ]
    }
   ],
   "source": [
    "# Selecionando estudantes do segundo ano mais\n",
    "estudantes = root.xpath(\"//student[@year >= '2']/text()\")\n",
    "\n",
    "# Exibir os nomes \n",
    "print( estudantes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Selecionar o nome do *professor* de Estruturas de Dados (course: \"Data Structures\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Professor de Data Structures: ['Dr. Emily Clark']\n"
     ]
    }
   ],
   "source": [
    "# Selecionar o professor \n",
    "professor_data_structures = root.xpath(\"//course[title='Data Structures']/professor/text()\")\n",
    "\n",
    "# Exibir o nome do professor\n",
    "print(\"Professor de Data Structures:\", professor_data_structures)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Selecionar os títulos de todos os *cursos* ofertados pelo departamento de Ciência da Computação (department: Computer Science);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Introduction to Computer Science', 'Data Structures']\n"
     ]
    }
   ],
   "source": [
    "# títulos dos cursos do departamento de ciência da computação\n",
    "cursos_cs = root.xpath(\"//department[@name='Computer Science']/course/title/text()\")\n",
    "\n",
    "# Exibir os títulos \n",
    "print(cursos_cs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Selecionar os nomes de todos os *departamentos* que sejam pertencentes à Escola de Engenharia (college: Engineering)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Computer Science', 'Mechanical Engineering']\n"
     ]
    }
   ],
   "source": [
    "# Selecionar os nomes dos departamentos pertencentes à Escola de Engenharia\n",
    "departamentos_engineering = root.xpath(\"//college[@name='Engineering']/department/@name\")\n",
    "\n",
    "# Exibir os nomes dos departamentos\n",
    "print(departamentos_engineering)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercício 6:\n",
    "\n",
    "Utilize o arquivo XML em anexo e a biblioteca `lxml` com seletores de CSS para:\n",
    "\n",
    "Selecionar os títulos de todos os cursos cujos professores possuem estabilidade (tenure);\n",
    "Selecionar os títulos de todos os cursos que possuem horário de início pela manhã (AM). Dica: cuidado com nomes antigos de pseudo-classes, caso algum não funcione tente o nome antigo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curso com professor com estabilidade (tenure): Introduction to Computer Science\n",
      "Curso com professor com estabilidade (tenure): Thermodynamics\n",
      "Curso com professor com estabilidade (tenure): World History\n",
      "Curso matutino: Introduction to Computer Science\n",
      "Curso matutino: Thermodynamics\n",
      "Curso matutino: World History\n",
      "Curso matutino: Ethics\n"
     ]
    }
   ],
   "source": [
    "# Selecionar os títulos dos cursos com professores com estabilidade \n",
    "cursos_tenure = root.cssselect('course professor[tenure=\"true\"] ~ title')\n",
    "\n",
    "# Verificar e exibir os títulos dos cursos com professores\n",
    "if cursos_tenure:\n",
    "    for curso in cursos_tenure:\n",
    "        print(\"Curso com professor com estabilidade (tenure):\", curso.text)\n",
    "else:\n",
    "    print(\"Nenhum curso com professor com estabilidade foi encontrado\")\n",
    "\n",
    "# Buscar dentro do elemento time dentro do schedule\n",
    "cursos_am = root.cssselect('course schedule time')\n",
    "\n",
    "# Verificar se encontramos horários que contenham AM\n",
    "if cursos_am:\n",
    "    for time_element in cursos_am:\n",
    "        if \"AM\" in time_element.text:\n",
    "            title_element = time_element.xpath(\"../../title\")[0]  # Volta dois níveis para encontrar o título do curso\n",
    "            print(\"Curso matutino:\", title_element.text)\n",
    "else:\n",
    "    print(\"Nenhum curso matutino foi encontrado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercício 7: \n",
    "\n",
    "Examine um site de sua escolha na lista de sites fornecida em anexo e descubra o padrão de URL para paginação que ele aceita. Então, utilize-o para obter uma lista de links de notícias requisitando as 2 primeiras páginas e raspando os links de cada uma através de um único seletor de CSS aplicado via `BeautifulSoup`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Para este exercício o site escolhido foi: Rondonia Dinâmica em https://www.rondoniadinamica.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Link encontrado: https://www.rondoniadinamica.com/noticias/2024/09/euma-tourinho-tem-48-horas-para-publicar-direito-de-resposta-de-mariana-carvalho-multa-pode-chegar-a-r-15-mil-caso-de-descumprimento,200736.shtml\n",
      "Link encontrado: https://www.rondoniadinamica.com/noticias/2024/09/governo-intensifica-acoes-de-emergencia-nas-comunidades-indigenas-de-vilhena-e-guajara-mirim,200735.shtml\n",
      "Link encontrado: https://www.rondoniadinamica.com/noticias/2024/09/eleicao-no-sindicato-dos-vigilantes-de-rondonia-ocorre-dentro-da-normalidade-e-resultado-so-confirma-os-anteriores,200734.shtml\n",
      "Link encontrado: https://www.rondoniadinamica.com/noticias/2024/09/policia-civil-de-rondonia-deflagra-operacao-asfixia-em-cerejeiras,200733.shtml\n",
      "Link encontrado: https://www.rondoniadinamica.com/noticias/2024/09/regional-cafe-lanca-projeto-sintero-interativo-iniciativa-visa-facilitar-atendimentos-presenciais,200732.shtml\n",
      "Link encontrado: https://www.rondoniadinamica.com/noticias/2024/09/dino-marca-para-10-de-outubro-audiencia-para-debater-orcamento-secreto,200731.shtml\n",
      "Link encontrado: https://www.rondoniadinamica.com/noticias/2024/09/prazo-para-censo-previdenciario-2024-e-prorrogado-ate-31-de-outubro,200730.shtml\n",
      "Link encontrado: https://www.rondoniadinamica.com/noticias/2024/09/obras-de-revitalizacao-da-quadra-portelinha-ultrapassam-90-de-execucao-em-cujubim,200729.shtml\n",
      "Link encontrado: https://www.rondoniadinamica.com/noticias/2024/09/prorrogadas-inscricoes-no-premio-conciliar-e-legal-podem-ser-feitas-ate-8-de-novembro,200728.shtml\n",
      "Link encontrado: https://www.rondoniadinamica.com/noticias/2024/09/apos-debate-na-tv-expectativa-pelas-pesquisas-passagem-de-bolsonaro-influenciou-morre-pedro-andre-da-folha,200727.shtml\n",
      "Link encontrado: https://www.rondoniadinamica.com/noticias/2024/09/vilhenense-compoe-equipe-de-brasilia-para-disputar-campeonato-nacional-sub-15-de-basquete,200726.shtml\n",
      "Link encontrado: https://www.rondoniadinamica.com/noticias/2024/09/outubro-rosa-abertas-as-inscricoes-para-a-carreta-itinerante-do-hospital-de-amor,200725.shtml\n",
      "Link encontrado: https://www.rondoniadinamica.com/noticias/2024/09/dia-mundial-do-coracao-programa-hiperdia-oferece-suporte-para-hipertensos-e-diabeticos-em-porto-velho,200724.shtml\n",
      "Link encontrado: https://www.rondoniadinamica.com/noticias/2024/09/alimentos-arrecadados-na-gincana-do-conhecimento-da-ale-ro-sao-destinados-a-apae,200723.shtml\n",
      "Link encontrado: https://www.rondoniadinamica.com/noticias/2024/09/mp-funcionara-em-regime-de-plantao-nesta-quarta-feira-em-porto-velho,200722.shtml\n",
      "Link encontrado: https://www.rondoniadinamica.com/noticias/2024/09/mariana-carvalho-defende-a-concessao-de-beneficios-fiscais-para-revitalizacao-do-centro-,200721.shtml\n",
      "Link encontrado: https://www.rondoniadinamica.com/noticias/2024/09/cirone-deiro-anuncia-mais-asfalto-para-avenida-sete-de-setembro-em-cacoal,200720.shtml\n",
      "Link encontrado: https://www.rondoniadinamica.com/noticias/2024/09/participacao-na-olimpiada-dos-tribunais-confirma-acerto-do-tce-ro-em-valorizar-o-servidor,200719.shtml\n",
      "Link encontrado: https://www.rondoniadinamica.com/noticias/2024/09/tjro-e-destaque-no-ii-premio-qualidade-da-informacao-contabil-e-fiscal,200718.shtml\n",
      "Link encontrado: https://www.rondoniadinamica.com/noticias/2024/09/eleitores-nao-podem-ser-presos-a-partir-desta-terca,200717.shtml\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Função para extrair os links de uma página\n",
    "def extrair_links(url):\n",
    "    # Fazer a requisição http com requests\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Verificar se está ok\n",
    "    if response.status_code == 200:\n",
    "        # Criar o objeto com BS\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Selecionar os links das noticias com o seletor CSS\n",
    "        links_noticias = soup.select('a.post-title[href]')\n",
    "        \n",
    "        # Adicionar a url base para transformar links relativos em completos\n",
    "        base_url = \"https://www.rondoniadinamica.com\"\n",
    "        return [base_url + link['href'] if not link['href'].startswith(base_url) else link['href'] for link in links_noticias]\n",
    "    else:\n",
    "        print(f\"Erro ao acessar a  {url}\")\n",
    "        return []\n",
    "\n",
    "# url das páginas 1 e 2 conforme o padrão de paginação\n",
    "url_pagina_1 = \"https://www.rondoniadinamica.com/ultimas-noticias?pagina=1\"\n",
    "url_pagina_2 = \"https://www.rondoniadinamica.com/ultimas-noticias?pagina=2\"\n",
    "\n",
    "# Extrair os links das páginas\n",
    "links_pagina_1 = extrair_links(url_pagina_1)\n",
    "links_pagina_2 = extrair_links(url_pagina_2)\n",
    "\n",
    "# Combinando os links\n",
    "todos_os_links = links_pagina_1 + links_pagina_2\n",
    "\n",
    "# Exibir os links \n",
    "for link in todos_os_links:\n",
    "    print(\"Link encontrado:\", link)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercício 8:\n",
    "\n",
    "Faça um loop para os 3 primeiros links da lista obtida na questão anterior requisitando o HTML de cada página com a biblioteca que preferir (`urllib`, `requests`, etc.) e aplicando funções baseadas em `BeautifulSoup` para capturar e por fim salvar em um mesmo arquivo JSON, junto à URL de cada notícia e ao datetime do momento da requisição de cada página:\n",
    "\n",
    "* O objeto datetime (timezone-aware) da data e hora da publicação da notícia;\n",
    "* O título da notícia;\n",
    "* O corpo do texto da notícia;\n",
    "* O subtítulo da notícia (se houver);\n",
    "* O autor ou autores da notícia (se houver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Informações extraídas e salvas em JSON.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Função para extrair dados da notícia\n",
    "def extrair_dados_noticia(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Capturando o título da notícia\n",
    "    titulo = soup.find('strong').text.strip() if soup.find('strong') else 'Título não encontrado'\n",
    "\n",
    "    # Capturando o autor da notícia\n",
    "    autor = soup.find('div', class_='post-excerpt').find('strong').text.strip() if soup.find('div', class_='post-excerpt') else 'Autor desconhecido'\n",
    "\n",
    "    # Capturando a data de publicação\n",
    "    data_publicacao_element = soup.find('div', class_='post-meta')\n",
    "    data_publicacao = data_publicacao_element.find_all('strong')[1].text.strip() if data_publicacao_element and len(data_publicacao_element.find_all('strong')) > 1 else 'Data não encontrada'\n",
    "    \n",
    "    # Tentativa de converter a dada\n",
    "    try:\n",
    "        data_publicacao_dt = datetime.strptime(data_publicacao, '%d/%m/%Y às %Hh%M').isoformat() if data_publicacao != 'Data não encontrada' else 'Data inválida'\n",
    "    except ValueError:\n",
    "        data_publicacao_dt = 'Data inválida'\n",
    "\n",
    "    # Capturando o corpo da notícia\n",
    "    corpo = ' '.join([p.text.strip() for p in soup.find_all('p')]) if soup.find_all('p') else 'Corpo da notícia não encontrado'\n",
    "\n",
    "    return {\n",
    "        'url': url,\n",
    "        'titulo': titulo,\n",
    "        'autor': autor,\n",
    "        'data_publicacao': data_publicacao_dt,\n",
    "        'corpo': corpo\n",
    "    }\n",
    "\n",
    "# Testando os links com a função\n",
    "links_noticias = [\n",
    "    \"https://www.rondoniadinamica.com/noticias/2024/09/pesquisa-da-futura-inteligencia-mostra-que-havera-segundo-turno-pois-mariana-teria-apenas-454-dos-votos,200659.shtml\",\n",
    "    \"https://www.rondoniadinamica.com/noticias/2024/09/ladrao-de-moto-troca-tiros-com-a-pm-apos-assalto-na-zona-leste,200658.shtml\",\n",
    "    \"https://www.rondoniadinamica.com/noticias/2024/09/jovem-e-alvejado-a-tiros-por-grupo-em-carro-residencial-porto-madero-,200657.shtml\"\n",
    "]\n",
    "\n",
    "# Processando os 3 primeiros links e salvando em um JSON\n",
    "noticias_dados = [extrair_dados_noticia(link) for link in links_noticias]\n",
    "\n",
    "# Salvando os dados no arquivo JSON\n",
    "with open('noticias_extracao_clean.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(noticias_dados, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Informações extraídas e salvas em JSON.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercício 10:\n",
    "\n",
    "Extraia uma lista de empregos do site https://br.indeed.com. Extraia os títulos dos empregos da primeira página de resultados ao pesquisar por \"Data Scientist\" na área da capital de seu estado. O site usa JavaScript para carregar as listas dinamicamente, o que significa que você não pode recuperar esses dados simplesmente usando solicitações ou BeautifulSoup. Escreva um script em Python usando Selenium para extrair os títulos dos empregos desta página junto a outras informações que você considere relevante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Scientist\n",
      "Fellow Data Scientist\n",
      "Data Scientist\n",
      "Senior Data Scientist\n",
      "Graduate 2024 Data Scientist, Brazil\n",
      "DATA SCIENTIST III - AVALIACAO INDEPENDENTE DE MODELOS\n",
      "2024 Graduate Data Scientist for Risk and Fraud, Brazil\n",
      "Data Scientist\n",
      "CIENTISTA JR\n",
      "Data Scientist Internship, Brazil – BCG X\n",
      "Data Scientist\n",
      "Data scientist specialist - infrastructure & operations\n",
      "Data Scientist / Analytics Specialist\n",
      "Data Scientist Pleno\n",
      "AIOPs IT Analyst II - Data Scientist\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "# Defina o caminho para o ChromeDriver\n",
    "chrome_driver_path = 'C:/Program Files/chromedriver-win64/chromedriver.exe'  # Ajuste o caminho\n",
    "\n",
    "# Inicialize o WebDriver com o Service\n",
    "service = Service(executable_path=chrome_driver_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# Abra a página do Indeed com a pesquisa para \"Data Scientist\" em São Paulo\n",
    "url = 'https://br.indeed.com/jobs?q=data+scientist&l=Estado+de+S%C3%A3o+Paulo&from=searchOnHP&vjk=857da5800d4b2f83'\n",
    "driver.get(url)\n",
    "\n",
    "# Espere um pouco para garantir que a página carregue completamente\n",
    "time.sleep(5)\n",
    "\n",
    "# Extraia os títulos dos empregos\n",
    "titulos_empregos = driver.find_elements(By.CSS_SELECTOR, 'h2.jobTitle span[title]')\n",
    "\n",
    "# Imprima os títulos extraídos\n",
    "for titulo in titulos_empregos:\n",
    "    print(titulo.text)\n",
    "\n",
    "# Manter a janela aberta (remova o comando de fechamento)\n",
    "# driver.quit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Título: Data Scientist\n",
      "Empresa: Empresa não informada\n",
      "Localização: Localização não informada\n",
      "\n",
      "Título: Fellow Data Scientist\n",
      "Empresa: Empresa não informada\n",
      "Localização: Localização não informada\n",
      "\n",
      "Título: Data Scientist\n",
      "Empresa: Empresa não informada\n",
      "Localização: Localização não informada\n",
      "\n",
      "Título: Senior Data Scientist\n",
      "Empresa: Empresa não informada\n",
      "Localização: Localização não informada\n",
      "\n",
      "Título: Graduate 2024 Data Scientist, Brazil\n",
      "Empresa: Empresa não informada\n",
      "Localização: Localização não informada\n",
      "\n",
      "Título: DATA SCIENTIST III - AVALIACAO INDEPENDENTE DE MODELOS\n",
      "Empresa: Empresa não informada\n",
      "Localização: Localização não informada\n",
      "\n",
      "Título: 2024 Graduate Data Scientist for Risk and Fraud, Brazil\n",
      "Empresa: Empresa não informada\n",
      "Localização: Localização não informada\n",
      "\n",
      "Título: Data Scientist\n",
      "Empresa: Empresa não informada\n",
      "Localização: Localização não informada\n",
      "\n",
      "Título: DATA SCIENTIST III\n",
      "Empresa: Empresa não informada\n",
      "Localização: Localização não informada\n",
      "\n",
      "Título: CIENTISTA JR\n",
      "Empresa: Empresa não informada\n",
      "Localização: Localização não informada\n",
      "\n",
      "Título: Data Scientist Internship, Brazil – BCG X\n",
      "Empresa: Empresa não informada\n",
      "Localização: Localização não informada\n",
      "\n",
      "Título: Data Scientist\n",
      "Empresa: Empresa não informada\n",
      "Localização: Localização não informada\n",
      "\n",
      "Título: DATA SCIENTIST III (JURÍDICO)\n",
      "Empresa: Empresa não informada\n",
      "Localização: Localização não informada\n",
      "\n",
      "Título: AIOPs IT Analyst II - Data Scientist\n",
      "Empresa: Empresa não informada\n",
      "Localização: Localização não informada\n",
      "\n",
      "Título: Data scientist specialist - infrastructure & operations\n",
      "Empresa: Empresa não informada\n",
      "Localização: Localização não informada\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Configuração do WebDriver (ajuste o caminho conforme sua instalação do ChromeDriver)\n",
    "chrome_driver_path = 'C:/Program Files/chromedriver-win64/chromedriver.exe'\n",
    "service = Service(chrome_driver_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# Abra a página do Indeed com a pesquisa para \"Data Scientist\" em São Paulo\n",
    "url = 'https://br.indeed.com/jobs?q=data+scientist&l=Estado+de+S%C3%A3o+Paulo'\n",
    "driver.get(url)\n",
    "\n",
    "# Aguarda o carregamento da página\n",
    "time.sleep(3)\n",
    "\n",
    "# Coleta o HTML da página\n",
    "html = driver.page_source\n",
    "\n",
    "# Utiliza o BeautifulSoup para analisar o HTML\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Extrai informações dos empregos\n",
    "jobs = soup.find_all('div', class_='job_seen_beacon')\n",
    "\n",
    "for job in jobs:\n",
    "    title = job.find('h2', class_='jobTitle').text.strip()\n",
    "    company = job.find('span', class_='companyName').text.strip() if job.find('span', class_='companyName') else 'Empresa não informada'\n",
    "    location = job.find('div', class_='companyLocation').text.strip() if job.find('div', class_='companyLocation') else 'Localização não informada'\n",
    "    print(f'Título: {title}\\nEmpresa: {company}\\nLocalização: {location}\\n')\n",
    "\n",
    "# Fechar o WebDriver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Título': 'Data Scientist', 'Localização': 'São Paulo, SP', 'Link': 'https://br.indeed.com/rc/clk?jk=c3d2208cbc32726e&bb=-LH1_Ab1oJCFrzauwtsHrJu3t5l9PqTpMw4c3GDp_UiZqfIPVPVQSN_6SvwYorEWoHkZBNUyHRBIVEB0pIhWWfnqbAO88wkelvD_-rk00omKHtXOfNzrz9TfxqjMESiZ&xkcb=SoCx67M37iB_Qbwxrx0LbzkdCdPP&fccid=248e71f02a698bfc&vjs=3'}, {'Título': 'Fellow Data Scientist', 'Localização': 'São Paulo, SP', 'Link': 'https://br.indeed.com/rc/clk?jk=38b5d12a90da1b37&bb=-LH1_Ab1oJCFrzauwtsHrOdjT8fBKSsq3_VXi-rJKw0a8HvrjShffiCpyDYRym5ssStYZZuNvENVlxafjk5aW4-NzQEezu3kBQs55xB94X7ZidLhp4XM8zkhVUj5XJoM&xkcb=SoAF67M37iB_Qbwxrx0KbzkdCdPP&fccid=53fac3ef8e00e16f&vjs=3'}, {'Título': 'Data Scientist', 'Localização': 'São Paulo, SP', 'Link': 'https://br.indeed.com/rc/clk?jk=618962f8181ff75e&bb=-LH1_Ab1oJCFrzauwtsHrNWiHtyLF6KQKtRfl3DlvZqx9WxZEagFT9InIMeRFYO87A0Jd_B9f22CHSO4VB89ZcZBiV6QJsLt-wAw7SK8IX9o_BXpnLAMr1tgGky_Yw2I&xkcb=SoCY67M37iB_Qbwxrx0JbzkdCdPP&fccid=0b476c4a7add47fe&vjs=3'}, {'Título': 'Senior Data Scientist', 'Localização': 'São Paulo, SP', 'Link': 'https://br.indeed.com/rc/clk?jk=b9115aad718d12e8&bb=-LH1_Ab1oJCFrzauwtsHrFQSguR5bn6fKku8G-yse4CnSa4TMzcArLIYrl0ZqFyW8Ju9cCzZilwN5QAlFkqBRTOgEZC91rTxtCCdc-v7Xk1PCwMM6HkG0LGsiNLZHeBC&xkcb=SoAs67M37iB_Qbwxrx0IbzkdCdPP&fccid=bc90041946a5fcc9&vjs=3'}, {'Título': 'Graduate 2024 Data Scientist, Brazil', 'Localização': 'São Paulo, SP', 'Link': 'https://br.indeed.com/rc/clk?jk=37e710ba7a232648&bb=-LH1_Ab1oJCFrzauwtsHrAOPEIHbw6BKQM5bnTWOr88aFZXLWCDcar6yqLsD1hqMpkneqjMiGOTBsBm2vOdrlrgqpCZbPWPqAQCle2XdJlmYpaKASkUUyd0EyPgLLP2-&xkcb=SoCi67M37iB_Qbwxrx0PbzkdCdPP&fccid=f766f8bfbc3effb7&vjs=3'}, {'Título': 'DATA SCIENTIST III - AVALIACAO INDEPENDENTE DE MODELOS', 'Localização': 'Osasco, SP', 'Link': 'https://br.indeed.com/rc/clk?jk=b596397eb7eedb13&bb=-LH1_Ab1oJCFrzauwtsHrAuiHzVLwv6NC2gJNR7D2qkLQ4FWlBT34BoEztomwKCDtf5jR4WiUAwiPe26tE_zpCqzXSr9ExkpnIX8x6h0XeYumkr3G-LFUe9uYNa0ISRj&xkcb=SoAW67M37iB_Qbwxrx0ObzkdCdPP&fccid=dd708ee3271585de&vjs=3'}, {'Título': '2024 Graduate Data Scientist for Risk and Fraud, Brazil', 'Localização': 'São Paulo, SP', 'Link': 'https://br.indeed.com/rc/clk?jk=ed6c697dd6d80298&bb=-LH1_Ab1oJCFrzauwtsHrHCi-L7UwYBhuQ2fXKSshpRUIhonUay_KQfwJXh_XybWmeE4Rv6R2bzD2HQwoWvqBzu2olnbRFc6sZFuHJY0iL91E5-bPYmmq1s-FRc4uN4s&xkcb=SoCL67M37iB_Qbwxrx0NbzkdCdPP&fccid=f766f8bfbc3effb7&vjs=3'}, {'Título': 'Data Scientist', 'Localização': 'São Paulo, SP', 'Link': 'https://br.indeed.com/rc/clk?jk=9c6fa99646f12bd6&bb=-LH1_Ab1oJCFrzauwtsHrDqOJ9gJGLooJKcS1RQimo8nIsHSen6Rqrufq8FDGFDU0JbC3TDN7vsviriSmP-fM4oVjvoHy2A_oo12P3xaKbbKh6nZZNfv-2A0Aoo8sIOo&xkcb=SoA_67M37iB_Qbwxrx0MbzkdCdPP&fccid=2263de16121c2d82&vjs=3'}, {'Título': 'DATA SCIENTIST III', 'Localização': 'Osasco, SP', 'Link': 'https://br.indeed.com/rc/clk?jk=7889f78735d9d15d&bb=-LH1_Ab1oJCFrzauwtsHrIJoeyC0eGxyAczXAq93RDm5ut7OlzgFKjdB_XJWlvQBN4a96qvnOJRT1a1U776B0ufdkRsYuJ_yCvTIdHjrJgg25y4PYUzsnqx98uJJNpqO&xkcb=SoDW67M37iB_Qbwxrx0DbzkdCdPP&fccid=dd708ee3271585de&vjs=3'}, {'Título': 'CIENTISTA JR', 'Localização': 'São José dos Campos, SP', 'Link': 'https://br.indeed.com/rc/clk?jk=c558b1c23faaacdc&bb=-LH1_Ab1oJCFrzauwtsHrCiOgmSG2bKuHmVxPemiSyAfIJ-hZwHsAxD7BWWDXDs-PZoZ48VjW-fr1UIqPRcPa47P3QhSeFKExzwkm8HWTlwkqLee6-IEXyv2qO30MBo0&xkcb=SoBi67M37iB_Qbwxrx0CbzkdCdPP&fccid=ed22db47960d00a3&vjs=3'}, {'Título': 'Data Scientist Internship, Brazil – BCG X', 'Localização': 'São Paulo, SP', 'Link': 'https://br.indeed.com/rc/clk?jk=6a5c18c7a1c32ca9&bb=-LH1_Ab1oJCFrzauwtsHrBhcZJ89QG2NTR9dPVcxMzpSsfl6pqfYOAwKNv9Ug3GtEII5f-AVmIhS7xDH5wwA9UuIobmN-Zd6c-hjdcHFillC6MORHrcUqM86WSkRAiAS&xkcb=SoD_67M37iB_Qbwxrx0BbzkdCdPP&fccid=b95f9eb47968fa13&vjs=3'}, {'Título': 'Data Scientist', 'Localização': 'São Paulo, SP', 'Link': 'https://br.indeed.com/rc/clk?jk=8e820284460324bc&bb=-LH1_Ab1oJCFrzauwtsHrOo7ED5HsJ4MYXb4rNvAjVAPsPm7_QjL3-k3sN8-oFa1d1iNdzIn31B6FCY_cmBEgM_PlMUtyJ5VLYGz5CClMHyWpQN7lVy-ZoTztQ9AfHqm&xkcb=SoBL67M37iB_Qbwxrx0AbzkdCdPP&fccid=af3399bd8ecfbafc&vjs=3'}, {'Título': 'DATA SCIENTIST III (JURÍDICO)', 'Localização': 'Osasco, SP', 'Link': 'https://br.indeed.com/rc/clk?jk=e896b57931a8ef21&bb=-LH1_Ab1oJCFrzauwtsHrOdjT8fBKSsqKeaVGF8KpmklK5A2cht8p71ZKtyzMAeKZS09jwSTVEFes9vchmYaBb8h0s29fD6WWK5-AFArJMnhkMSQ1rB7-HVsaFVWjRoc&xkcb=SoDF67M37iB_Qbwxrx0HbzkdCdPP&fccid=dd708ee3271585de&vjs=3'}, {'Título': 'AIOPs IT Analyst II - Data Scientist', 'Localização': 'São Paulo, SP', 'Link': 'https://br.indeed.com/rc/clk?jk=12cf7fb94b1dde9f&bb=-LH1_Ab1oJCFrzauwtsHrNhvdGBjYxxXYqDvM-4uvvyFGkHe1S5NPrSwnX1ECpeRsPDlEMj9CXpxo1r-SJPRRuLlLz2FIpyznrVDvQaTnZ9jfNlrJSgESg%3D%3D&xkcb=SoBx67M37iB_Qbwxrx0GbzkdCdPP&fccid=e57a47a3d1d90e39&vjs=3'}, {'Título': 'Data scientist specialist - infrastructure & operations', 'Localização': 'Indaiatuba, SP', 'Link': 'https://br.indeed.com/rc/clk?jk=61a3d1477b63b7db&bb=-LH1_Ab1oJCFrzauwtsHrDI6w9L3XvIKLziHfPHGXyatxbAkSfpzK5DMxZXLHUdmY8J1ay2U9VGhU96H_vN-q5wQcdQUTi4jFzMe4WM90PoVENp05htYI5cE4_8VyK0Y&xkcb=SoDs67M37iB_Qbwxrx0FbzkdCdPP&fccid=248e71f02a698bfc&vjs=3'}]\n",
      "Informações extraídas e salvas em JSON.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Definimdo o caminho do chromedriver \n",
    "chrome_driver_path = 'C:/Program Files/chromedriver-win64/chromedriver.exe'\n",
    "\n",
    "# Inicializando o webdriver\n",
    "service = Service(executable_path=chrome_driver_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# Abrir a página com os requisitos da pesquisa\n",
    "url = 'https://br.indeed.com/jobs?q=data+scientist&l=Estado+de+S%C3%A3o+Paulo'\n",
    "driver.get(url)\n",
    "\n",
    "# Tempo de espera para o carregamento 10 seg\n",
    "wait = WebDriverWait(driver, 10)\n",
    "\n",
    "# Aguardar que pelo menos um título de vaga \n",
    "vagas = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'div.slider_container')))\n",
    "\n",
    "lista_vagas = []\n",
    "\n",
    "for vaga in vagas:\n",
    "    try:\n",
    "        # Extrair o título da vaga e o link\n",
    "        titulo = vaga.find_element(By.CSS_SELECTOR, 'h2 a').text\n",
    "        link = vaga.find_element(By.CSS_SELECTOR, 'h2 a').get_attribute('href')\n",
    "    except:\n",
    "        titulo = \"Título não encontrado\"\n",
    "        link = None\n",
    "    \n",
    "    if link:\n",
    "        # Acessar a página da vaga para coletar mais informações\n",
    "        driver.get(link)\n",
    "        time.sleep(2) \n",
    "        \n",
    "        try:\n",
    "            localizacao = driver.find_element(By.CSS_SELECTOR, 'div#jobLocationText span').text\n",
    "        except:\n",
    "            localizacao = \"Localização não informada\"\n",
    "        \n",
    "        vaga_info = {\n",
    "            'Título': titulo,\n",
    "            'Localização': localizacao,\n",
    "            'Link': link\n",
    "        }\n",
    "        \n",
    "        lista_vagas.append(vaga_info)\n",
    "\n",
    "        # Voltar para a página inicial de listagem de vagas\n",
    "        driver.back()\n",
    "        time.sleep(2)\n",
    "\n",
    "# Fechar o navegador\n",
    "driver.quit()\n",
    "\n",
    "# Exibir as vagas coletadas\n",
    "print(lista_vagas)\n",
    "\n",
    "# Salvar as vagas em um arquivo JSON\n",
    "with open('vagas_data_scientist.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(lista_vagas, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Informações extraídas e salvas em JSON.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
